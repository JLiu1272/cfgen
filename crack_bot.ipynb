{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-free grammar text generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Clean and tag a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import glob\n",
    "import random\n",
    "# import cPickle as pickle\n",
    "import pickle\n",
    "  \n",
    "path = 'revelation/revelation.txt'   \n",
    "corpus_members=glob.glob(path)\n",
    "corpus = ''\n",
    "# get rid of random line breaks and exclude troublesome expressions like quotes\n",
    "for member in corpus_members:\n",
    "    with open (member, \"r\") as openfile:\n",
    "        data = openfile.read()\n",
    "        for badchar in ['\\t','\\n','-\\n','\\'','\\\"','`','|','--']:\n",
    "            data = data.replace(badchar, ' ')\n",
    "        data = data.replace('.','. ')\n",
    "        data = data.replace(',',', ')\n",
    "        data = data.replace(';','; ')\n",
    "        data = data.replace(':',': ')\n",
    "    corpus = corpus + ' '+ data\n",
    "tokens = nltk.word_tokenize(corpus)\n",
    "\n",
    "# looks at each word in the context of sentence and tags it\n",
    "pos_tagged_tokens = nltk.pos_tag(tokens)\n",
    "pickle.dump( pos_tagged_tokens, open( \"revelation/revelation_tagged.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record all terminal characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('revelation/revelation_tagged.pkl', 'rb') as handle:\n",
    "    pos_tagged_tokens = pickle.load(handle)\n",
    "\n",
    "# clear file\n",
    "open('revelation/revelation_rules.txt', 'w').close()\n",
    "\n",
    "file = open('revelation/revelation_rules.txt', 'a')\n",
    "\n",
    "# need to come up with some valid rules\n",
    "# tree = '''S -> NP VP\n",
    "# PP -> IN NP\n",
    "# NP -> DT NN | DT NN PP\n",
    "# VP -> VB NP | VP PP \\n'''\n",
    "# file.write(tree)\n",
    "\n",
    "tags = list({tupe[1] for tupe in pos_tagged_tokens})\n",
    "# dollar signs make everything go wrong\n",
    "#tags = [item.replace('$','x')for item in tags]\n",
    "badtags = ['#','$',',','-NONE-','.',':','TO','POS',\"''\"] # try to get NONE back if possible\n",
    "tags = [item for item in tags if item not in badtags]\n",
    "for tag in tags:\n",
    "#     if tag in ['#','$',',','-NONE-','.',':']:\n",
    "#         continue\n",
    "#     else:\n",
    "    allsyms = [('\\'' + tupe[0] + '\\'') for tupe in pos_tagged_tokens if tupe[1]== tag]\n",
    "    gr_rule = (tag + \" -> \")\n",
    "    gr_rule += ' | '.join(allsyms)\n",
    "    gr_rule += '\\n'\n",
    "    gr_rule = gr_rule.replace('PRP$','PRPx')\n",
    "    gr_rule = gr_rule.replace('WP$','WPx')\n",
    "    gr_rule = gr_rule.replace('-NONE-','xNONEx')\n",
    "    file.write(gr_rule)\n",
    "\n",
    "# specify these guys when creating grammar\n",
    "# still need to work with apostrophe, hypens ``\n",
    "file.write('''xperiod -> '.'\\n''')\n",
    "file.write('''xcomma -> ','\\n''')\n",
    "file.write('''xcolon -> ':'\\n''')\n",
    "file.write('''xsemicolon -> ';'\\n''')\n",
    "file.write('''openparen -> '('\\n''')\n",
    "file.write('''closeparen -> ')'\\n''')\n",
    "file.write('''xapostrophe -> \"\\'\"\\n''')\n",
    "file.write('''xquote -> \"''\"\\n''')\n",
    "\n",
    "# Save some compiling time\n",
    "file.write('''TO -> 'to'\\n''')\n",
    "\n",
    "# need to fix this\n",
    "#file.write('''POS -> \"\\'s\"\\n''')\n",
    "\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get Random sentence from corpus and print syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the Grammar Model\n",
      "Time: (6.78)s\n",
      "\n",
      "And there they shall be tortured day and night for timeless  ages.\n",
      "NPxSBARxS -> S S\n",
      "S -> CC NP NP VP\n",
      "CC -> 'and'\n",
      "NP -> EX\n",
      "EX -> 'there'\n",
      "NP -> PRP\n",
      "PRP -> 'they'\n",
      "VP -> MD VB NP\n",
      "MD -> 'shall'\n",
      "VB -> 'be'\n",
      "NP -> JJ NN\n",
      "JJ -> 'tortured'\n",
      "NN -> 'day'\n",
      "S -> CC VP xperiod\n",
      "CC -> 'and'\n",
      "VP -> NP PP\n",
      "NP -> NN\n",
      "NN -> 'night'\n",
      "PP -> IN NP\n",
      "IN -> 'for'\n",
      "NP -> JJ NN\n",
      "JJ -> 'timeless'\n",
      "NN -> 'ages'\n",
      "xperiod -> 'xperiod'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import CFG, ChartParser\n",
    "from random import choice\n",
    "\n",
    "from stat_parser import Parser\n",
    "parser = Parser()\n",
    "# pick a random sentence to parse\n",
    "# leave out the period at the end of the sentence\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "rsent = choice(tokenizer.tokenize(corpus))\n",
    "print (rsent)\n",
    "parsee=parser.parse(rsent)\n",
    "\n",
    "rules = \"\"\n",
    "to_replace = [',','.',':',';',\"''\",'(',')','$','+']\n",
    "replacements = ['xcomma','xperiod','xcolon','xsemicolon',\\\n",
    "                'xquote','openparen','closeparen','x','x']\n",
    " \n",
    "# possibly add: brackets, double quotes\n",
    "\n",
    "for production in parsee.productions():\n",
    "    rules += str(production) + '\\n'\n",
    "\n",
    "# now re-tag special characters\n",
    "swappairs = zip(to_replace, replacements)\n",
    "for member in swappairs:\n",
    "    rules = rules.replace(member[0],member[1])\n",
    "\n",
    "print (rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run grammar model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gramr = nltk.parse_cfg(\"\"\"S -> NP VP\n",
    "# ... PP -> P NP\n",
    "# ... NP -> Det N | Det N PP | 'I'\n",
    "# ... VP -> V NP | VP PP\n",
    "# ... Det -> 'an' | 'my'\n",
    "# ... N -> 'elephant' | 'pajamas'\n",
    "# ... V -> 'shot'\n",
    "# ... P -> 'in'\n",
    "# ... \"\"\")\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk import CFG, ChartParser, Nonterminal\n",
    "from random import choice\n",
    "\n",
    "def is_terminal(item):\n",
    "    \"\"\"\n",
    "    Check to see if a symbol is terminal\n",
    "    \"\"\"\n",
    "    return hasattr(item, '__hash__') and not isinstance(item, Nonterminal)\n",
    "\n",
    "def produce(grammar, symbol):\n",
    "    words = []\n",
    "    productions = grammar.productions(lhs = symbol)\n",
    "    production = choice(productions)\n",
    "    for sym in production.rhs():\n",
    "        if is_terminal(sym):\n",
    "            words.append(sym)\n",
    "        # recursion\n",
    "        else:\n",
    "            words.extend(produce(grammar, sym))\n",
    "    return words\n",
    "\n",
    "# read nonterminal rules from treebank file\n",
    "file = open('revelation/revelation_NTrules.txt', 'r')\n",
    "cfg_str = file.read()\n",
    "file.close()\n",
    "\n",
    "# read leaves from corpus\n",
    "file = open('revelation/revelation_rules.txt', 'r')\n",
    "cfg_str += '\\n' + file.read()\n",
    "file.close()\n",
    "\n",
    "grammar = CFG.fromstring('''\n",
    "S -> NP',' VP\n",
    "PP -> IN NP\n",
    "NP -> DT NN | DT NN PP | 'I'\n",
    "VP -> V NP | VP PP\n",
    "V -> 'shot' | 'killed' | 'wounded'\n",
    "DT -> 'an' | 'my'\n",
    "NN -> 'elephant'\n",
    "H -> 'unused'\n",
    "IN -> 'in' | 'outside'\n",
    "''')\n",
    "\n",
    "grammar = CFG.fromstring(cfg_str)\n",
    "\n",
    "parser = ChartParser(grammar)\n",
    "\n",
    "gr = parser.grammar()\n",
    "\n",
    "# make an S symbol to start the fun\n",
    "tgr = ChartParser(CFG.fromstring('''S -> NP VP''')).grammar()\n",
    "tgr.start()\n",
    "# gr.max_len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", dominates to purified up of These shone at , her credit at his blood and can twelve endurance of its everlasting be their angel , .\n",
      "and even will your authority was not upon the heat upon me from his hand .\n",
      "great shone throughout those harvest hand his thousand\n",
      "and been of earth cloud .\n",
      "they with fall Jesus what the double shot which own earth of the as she to the west to the quart are saw The someone upon A fellow handing who hate in you and hand Who have were them are created her presence of the point to be her stone , the statue were silk a front what which have to HARLOTS of mighty what are of the one-third , and any 1 kings xcolon the reward , or to our noises whose judgments sit will John was be to you to herself to sky , and can drown a voice to the harvest on Lamb heard given their hand Around the thousand . and any great bronze which have with the nor the holding to it which are need worshiped no anyone the shall try the forth statue to , great forth statue who earth sixth over her statue whose thunders are poured furnace blind song foot to its power into nor harm the right . are found your golden which call with the sea for There to will the complete roar are poured Alas shall these earth none shut I from their man to Jesus on Face for angel\n",
      "hast whose inhabitants sharp earth to single jewels of your sunlight which descending another trumpet , from The will salve the sound\n",
      "a blew reign all seashore of I against spotless of there\n",
      "and back scorch , their words .\n",
      "And even come the myrrh .\n",
      "fellow lampstands be fire this new honor whose players have shall entice the thousand like him become of his torture for the mixing of great kings or my book Who is to are was THE bird The man was never to there\n"
     ]
    }
   ],
   "source": [
    "#message = produce(gr, tgr.start())\n",
    "for ii in xrange(10):\n",
    "    print ' '.join(produce(gr, tgr.start())) \n",
    "\n",
    "\n",
    "# # tokenize by sentence and pick some random sentences to use?\n",
    "# # really need to get punctuation working\n",
    "# from stat_parser import Parser\n",
    "# parser = Parser()\n",
    "# kk=parser.parse(\"At the end of the nineteenth century the well-tested mechanical principles of Isaac Newton were the very heart of physical theory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a grammar file from Treebank"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# problem: searching for only identifiable tags is dropping clause symbols, etc. \n",
    "# But if a terminal symbo is missing, an IndexError will be raised\n",
    "# instead just make an lhs list and check for membership?\n",
    "\n",
    "rule_list = ''\n",
    "gram_leaves = [loc.rhs() for loc in list(tbank_productions)]\n",
    "gram_heads = [loc.lhs() for loc in list(tbank_productions)]\n",
    "gram_list= list(tbank_productions)\n",
    "for thing in gram_list:\n",
    "    thing_head = thing.lhs()\n",
    "    thing_leave = thing.rhs()\n",
    "    if str(thing_head) in tags:\n",
    "        #print str(thing_head)\n",
    "        continue\n",
    "    else:\n",
    "        message = str(thing_head) + \" -> \"\n",
    "        collection = [str(thing2) for thing2 in thing_leave]\n",
    "        for item in collection:\n",
    "            message += \" | \" + str(item)\n",
    "            rule_list += '\\n'+message\n",
    "\n",
    "    \n",
    "# # Get rid of dead heads\n",
    "# new_rule_list= ''\n",
    "# for line in rule_list.splitlines():\n",
    "#     if ' -> ' not in line[-5:]:\n",
    "#         new_rule_list +=line+'\\n'\n",
    "        \n",
    "rule_list=new_rule_list\n",
    "\n",
    "#get ride of first OR\n",
    "rule_list = rule_list.replace(' ->  |', ' ->  ')\n",
    "\n",
    "print rule_list\n",
    "# file = open('treebank_rules.txt', 'w')\n",
    "# file.write(rule_list)\n",
    "# file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parser = ChartParser(tbank_grammar)\n",
    "gr = parser.grammar()\n",
    "print ' '.join(produce(gr, gr.start()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
