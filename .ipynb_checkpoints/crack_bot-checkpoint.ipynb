{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-free grammar text generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Pack everything up nice and neatly, get generator working as well as it can **\n",
    "\n",
    "+ Why are there weird things happening with the periods?\n",
    "+ DEAL WITH EMPTY PRODUCTIONS IN GRAMMAR\n",
    "+ Fix the weird double period/comma problem, figure out where that's coming from.\n",
    "+ When stripping corpus, delete lines that are shorter than most (likely chapter headings)\n",
    "+ feed a slightly modified corpus into the kgram dictionary generator in order to account for periods and commas and other punctuation\n",
    "+ Need to identify words that are capital both when they start the sentence and whenever they appear (first names). Maybe make a list of all words that appear in both capital and non capital form?\n",
    "\n",
    "+ If pyStatParser fails, select sentence rules from a library of already-processed sentences\n",
    "\n",
    "+ delete parentheses, get punctuation to go into the original sentence structure in parse_sent()\n",
    "\n",
    "**Research ideas**\n",
    "+ generate text using words from one author and syntax from a different author (style swapper)\n",
    "+ can I 1. observe zipf's law in Frankenstein, and 2. observe it in text generated from Markov and CFG markov?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cfgen import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the heart was away the country away .\n",
      "\n",
      "\n",
      "the sprung spoke off the window then .\n",
      "\n",
      "\n",
      "another world sought off the suffice back .\n",
      "\n",
      "\n",
      "----------\n",
      "\n",
      "the bridge had up all murder burst .\n",
      "\n",
      "\n",
      "HIT!\n",
      "the design was away the fiend not .\n",
      "\n",
      "\n",
      "HIT!\n",
      "the falsehood was away the fiend not .\n",
      "\n",
      "\n",
      "----------\n",
      "\n",
      "and which will is done to knows its wasting nor qualities, in that to is barred to affords my diabolical on my falling I evils, at all abhorrence is which to find my transcendent ,. my returning thou events but my I spurred\n",
      "\n",
      "\n",
      "his remorse had filled upon your victim\n",
      "\n",
      "\n",
      "farewell fairly, which countenance I to know that vessel england be would hoarser penetrate found of vice\n",
      "\n",
      "\n",
      "----------\n",
      "\n",
      "HIT!\n",
      "my reason: my I for a I at craving of his destruction I is pierced into release .\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-67c17a8bb34a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Pick sentence structure randomly with Markov-biased selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0msome_txt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmycorp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtermrules_mycorp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_kgram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0msome_txt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_output_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msome_txt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msome_txt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/william/python_files/cfgen/cfgen.py\u001b[0m in \u001b[0;36mmake_sentence\u001b[0;34m(corpus, term_rules, *args, **kwargs)\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0mgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkov_flag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m                 \u001b[0mout_txt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproduce_kgram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstartpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkgram_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0mout_txt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstartpt\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mmaxdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/william/python_files/cfgen/cfgen.py\u001b[0m in \u001b[0;36mproduce_kgram\u001b[0;34m(grammar, symbol, kgram_dict, depth, maxdepth, sent)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msym\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproduce_kgram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msym\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkgram_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/william/python_files/cfgen/cfgen.py\u001b[0m in \u001b[0;36mproduce_kgram\u001b[0;34m(grammar, symbol, kgram_dict, depth, maxdepth, sent)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msym\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproduce_kgram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msym\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkgram_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/william/python_files/cfgen/cfgen.py\u001b[0m in \u001b[0;36mproduce_kgram\u001b[0;34m(grammar, symbol, kgram_dict, depth, maxdepth, sent)\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0mproduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproductions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m         \u001b[0mproduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproductions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msym\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/william/miniconda/envs/py2env/lib/python2.7/random.pyc\u001b[0m in \u001b[0;36mchoice\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;34m\"\"\"Choose a random element from a non-empty sequence.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# raises IndexError if seq is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# initialize all of the relevant variables\n",
    "mycorp = clean_corpus('/Users/william/python_files/cfgen/full_books/frankenstein_short.txt')\n",
    "tagged_corpus = tag_corpus(mycorp)\n",
    "termrules_mycorp = make_terminal_rules(tagged_corpus)\n",
    "my_kgram = make_kgram(mycorp, k=2)\n",
    "\n",
    "# testing various use cases of make_sentence()\n",
    "\n",
    "simple_sentence = 'The dog ran up the stairs slowly.'\n",
    "\n",
    "# Try parsing a fixed grammar with random words inserted for terminal symbols\n",
    "for ii in range(3):\n",
    "    some_txt = make_sentence(mycorp, termrules_mycorp, fixed_grammar=True, sample_sentence=simple_sentence)\n",
    "    some_txt = clean_output_text(some_txt)\n",
    "    print(some_txt)\n",
    "    print('\\n')\n",
    "    \n",
    "print(\"----------\\n\")\n",
    "\n",
    "# Try parsing a fixed grammar with Markov-biased selection of terminal words\n",
    "for ii in range(3):\n",
    "    some_txt = make_sentence(mycorp, termrules_mycorp, my_kgram, fixed_grammar=True, sample_sentence=simple_sentence)\n",
    "    some_txt = clean_output_text(some_txt)\n",
    "    print(some_txt)\n",
    "    print('\\n')\n",
    "\n",
    "print(\"----------\\n\")\n",
    "    \n",
    "\n",
    "# Pick sentence structure randomly as well\n",
    "for ii in range(3):\n",
    "    some_txt = make_sentence(mycorp, termrules_mycorp)\n",
    "    some_txt = clean_output_text(some_txt)\n",
    "    print(some_txt)\n",
    "    print('\\n')\n",
    "    \n",
    "print(\"----------\\n\")\n",
    "    \n",
    "# Pick sentence structure randomly with Markov-biased selection\n",
    "for ii in range(3):\n",
    "    some_txt = make_sentence(mycorp, termrules_mycorp, my_kgram)\n",
    "    some_txt = clean_output_text(some_txt)\n",
    "    print(some_txt)\n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.00017946877243359656, 0.00018001800180018, 0.00036744442403086535, 0.0]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_gscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.038787325670935173,\n",
       " 0.040584091492456432,\n",
       " 0.04047001112283604,\n",
       " 0.037304280865870003]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_simscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.00051336166329178905, 0.00039928129367139149]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_simscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03675579322638145"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import mean,std\n",
    "mean(all_simscores)\n",
    "std(all_simscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-e2d0e5cf617c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtext_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_sentence_markov\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_kgram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0msimscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimilarity_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmycorp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mgscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrammar_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mall_simscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-2462de6ed775>\u001b[0m in \u001b[0;36msimilarity_score\u001b[0;34m(s1, s2, threshold_length)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mmax_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_len\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreshold_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mall_comm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_common_substring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreshold_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_comm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmax_sim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-2462de6ed775>\u001b[0m in \u001b[0;36mall_common_substring\u001b[0;34m(s1, s2, threshold_length)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                 \u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mthreshold_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Estimate the quality of the text statistically\n",
    "\n",
    "# make 1000 sentences\n",
    "\n",
    "# initialize all of the relevant variables\n",
    "# mycorp = clean_corpus('/Users/william/python_files/cfgen/full_books/frankenstein.txt')\n",
    "# tagged_corpus = tag_corpus(mycorp)\n",
    "# termrules_mycorp = make_terminal_rules(tagged_corpus)\n",
    "# my_kgram = make_kgram(mycorp, k=3)\n",
    "\n",
    "all_gscores = list()\n",
    "all_simscores = list()\n",
    "with open(\"3markov_score.txt\", 'wb') as myfile:\n",
    "    for ii in range(100):\n",
    "\n",
    "        text_sample = make_sentence_markov(my_kgram, 1000)\n",
    "\n",
    "        simscore = similarity_score(text_sample, mycorp)\n",
    "        gscore = grammar_score(text_sample)\n",
    "        all_simscores.append(simscore)\n",
    "        all_gscores.append(gscore)\n",
    "\n",
    "        myfile.write(str(simscore)+'\\t'+str(gscore)+\"\\n\")\n",
    "        print(ii)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10149732620320856"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_score(out, mycorp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05144385026737968"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_score(out, mycorp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing the generated text for repetitions, the number of common substrings between the genereated text and the corpus was computed and divided by the maximum possible value.\n",
    "\n",
    "In order to check the grammar of the generated text, the open-source library LanguageTool and its accompany Python API language-check were used to individually count the number of uniqe gramamtical errors in each sentence generated by the tool. For the output of cfgen, the original sentence from which the grammatical structure was parsed, and the number of \"original\" erros was subtrated from the number etected in the generated text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix (old code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jellyfish\n",
    "\n",
    "\n",
    "def originality_score(sentence, corpus):\n",
    "    '''\n",
    "    Return the \"originality\" of the sentence, normalized by its length\n",
    "    \n",
    "    sentence : str\n",
    "        A generated sentence\n",
    "        \n",
    "    corpus : str\n",
    "        A large body of text to compare against\n",
    "    '''\n",
    "    \n",
    "    mylen = len(corpus)\n",
    "    str_dist = jellyfish.damerau_levenshtein_distance(unicode(sentence), unicode(corpus))\n",
    "    \n",
    "    # 1 - (edit distance / length of the larger of the two strings)\n",
    "    \n",
    "    norm_str_dist = float(str_dist)/mylen\n",
    "    \n",
    "    return str_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "cdef double fr(double x) except? -2:\n",
    "    return x**2-x\n",
    "\n",
    "\n",
    "cdef double all_common_substring2(s1, s2,threshold_length=15):\n",
    "    \n",
    "    m = [[0] * (1 + len(s2)) for i in range(1 + len(s1))]\n",
    "    all_sub = list()\n",
    "    longest, x_longest = 0, 0\n",
    "    for x in range(1, 1 + len(s1)):\n",
    "        for y in range(1, 1 + len(s2)):\n",
    "            if s1[x - 1] == s2[y - 1]:\n",
    "                m[x][y] = m[x - 1][y - 1] + 1\n",
    "                if m[x][y] == threshold_length:\n",
    "                    longest = m[x][y]\n",
    "                    x_longest = x\n",
    "                    sout = s1[x_longest - longest: x_longest]\n",
    "                    all_sub.append(sout)\n",
    "            else:\n",
    "                m[x][y] = 0\n",
    "                \n",
    "    return all_sub\n",
    "\n",
    "print(fr(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "with open('file1.txt') as file_1,open('file2.txt') as file_2:\n",
    "    file1_data = file_1.read()\n",
    "    file2_data = file_2.read()\n",
    "    similarity_ratio = SequenceMatcher(None,file1_data,file2_data).ratio()\n",
    "    print similarity_ratio  #plagiarism detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # functions for processing text output\n",
    "\n",
    "# import language_check\n",
    "# from numpy import median, floor\n",
    "\n",
    "\n",
    "# def all_common_substring(s1, s2,threshold_length=15):\n",
    "#     '''\n",
    "#     Return a list of all substrings of a given length that two\n",
    "#     strings have in common\n",
    "    \n",
    "#     Based on standard code for solving the \"longest common substring\" problem\n",
    "    \n",
    "#     '''\n",
    "    \n",
    "#     m = [[0] * (1 + len(s2)) for i in range(1 + len(s1))]\n",
    "#     all_sub = list()\n",
    "#     longest, x_longest = 0, 0\n",
    "#     for x in range(1, 1 + len(s1)):\n",
    "#         for y in range(1, 1 + len(s2)):\n",
    "#             if s1[x - 1] == s2[y - 1]:\n",
    "#                 m[x][y] = m[x - 1][y - 1] + 1\n",
    "#                 if m[x][y] == threshold_length:\n",
    "#                     longest = m[x][y]\n",
    "#                     x_longest = x\n",
    "#                     sout = s1[x_longest - longest: x_longest]\n",
    "#                     all_sub.append(sout)\n",
    "#             else:\n",
    "#                 m[x][y] = 0\n",
    "#     return all_sub\n",
    "\n",
    "\n",
    "# def similarity_score(s1, s2, threshold_length='auto'):\n",
    "#     '''\n",
    "#     Compute the similarity between two strings based on the\n",
    "#     number of identical substrings of at least a given length\n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "    \n",
    "#     s1 : str\n",
    "#     s2 : str\n",
    "#         The two strings to compare\n",
    "        \n",
    "#     threshold_length : int\n",
    "#         The length for overlapping substrings to be significant\n",
    "#         If this is not specified, it is set to thrice the median\n",
    "#         length of words in the two strings\n",
    "        \n",
    "#     Returns\n",
    "#     -------\n",
    "    \n",
    "#     score : float\n",
    "#         The similarity score, a number between 0.0 and 1.0\n",
    "    \n",
    "#     '''\n",
    "#     if threshold_length=='auto':\n",
    "#         ave_word_len = median([len(item) for item in (s1 + ' ' + s2).split(' ')])\n",
    "#         threshold_length = int(3*ave_word_len)\n",
    "    \n",
    "#     min_len = max([len(s1), len(s2)])\n",
    "#     max_sim = floor(min_len/float(threshold_length))\n",
    "    \n",
    "#     all_comm = all_common_substring(s1, s2, threshold_length=threshold_length)\n",
    "    \n",
    "#     score = float(len(all_comm))/max_sim\n",
    "    \n",
    "#     return score\n",
    "\n",
    "# def grammar_score(some_text):\n",
    "#     '''\n",
    "#     Count the total number of errors in a text\n",
    "    \n",
    "#     Excludes cosmetic errors, like misuse of capitals, \n",
    "#     and instead focus on structural issues\n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     some_text : str\n",
    "#     '''\n",
    "#     tool = language_check.LanguageTool('en-US')\n",
    "#     matches = tool.check(some_text)\n",
    "\n",
    "#     structural_errors = list()\n",
    "#     for item in matches:\n",
    "#         if item.ruleId.find('WHITESPACE') != -1:\n",
    "#             continue\n",
    "#         elif item.ruleId.find('UPPERCASE') != -1:\n",
    "#             continue\n",
    "#         elif item.ruleId.find('LOWERCASE') != -1:\n",
    "#             continue\n",
    "#         elif item.ruleId.find('MORFOLOGIK_RULE_EN_US') != -1:\n",
    "#             continue\n",
    "#         elif item.ruleId.find('ENGLISH_WORD_REPEAT_BEGINNING_RULE') != -1:\n",
    "#             continue\n",
    "#         else:\n",
    "#             structural_errors.append(item)\n",
    "    \n",
    "#     error_score = float(len(structural_errors))/len(some_text)\n",
    "    \n",
    "#     return error_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class CFGen \n",
    "\n",
    "instance variables:\n",
    "    bad tags to substitute out then back in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CFGen:\n",
    "    '''\n",
    "    \n",
    "    k : int\n",
    "        The order of the Markov model\n",
    "    '''\n",
    "    exclusions = [] # global to the class by not user-facing\n",
    "    \n",
    "    def __init__(self, corpus, k):\n",
    "        self.corpus = name    # instance variable unique to each instance\n",
    "        self.k = k\n",
    "    \n",
    "        self.kgram = make_kgram(self.corpus, k=self.k)\n",
    "        self.tagged_corpus = tag_corpus(mycorp)\n",
    "        self.term_rules = make_terminal_rules(self.tagged_corpus)\n",
    "    \n",
    "    def generate():\n",
    "        return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
