{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-free grammar text generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Choose which corpora to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pathroot = 'corpora_and_rules/frankenstein/'\n",
    "filename_root = pathroot+'frankenstein'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Clean and tag a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import glob\n",
    "import random\n",
    "# import cPickle as pickle\n",
    "import pickle\n",
    "  \n",
    "path = filename_root+'.txt'   \n",
    "corpus_members=glob.glob(path)\n",
    "corpus = ''\n",
    "\n",
    "# get rid of random line breaks and exclude troublesome expressions like quotes\n",
    "for member in corpus_members:\n",
    "    with open (member, \"r\") as openfile:\n",
    "        data = openfile.read()\n",
    "        for badchar in ['\\t','\\n','-\\n','\\'','\\\"','`','|','--']:\n",
    "            data = data.replace(badchar, ' ')\n",
    "        data = data.replace('.','. ')\n",
    "        data = data.replace(',',', ')\n",
    "        data = data.replace(';','; ')\n",
    "        data = data.replace(':',': ')\n",
    "    corpus = corpus + ' '+ data\n",
    "tokens = nltk.word_tokenize(corpus)\n",
    "\n",
    "# looks at each word in the context of sentence and tags it\n",
    "pos_tagged_tokens = nltk.pos_tag(tokens)\n",
    "pickle.dump( pos_tagged_tokens, open( filename_root+\"_tagged.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record all terminal characters in a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(filename_root+\"_tagged.pkl\", 'rb') as handle:\n",
    "    pos_tagged_tokens = pickle.load(handle)\n",
    "\n",
    "# clear file\n",
    "open(filename_root+\"_rules.txt\", 'w').close()\n",
    "\n",
    "file = open(filename_root+\"_rules.txt\", 'a')\n",
    "\n",
    "tags = list({tupe[1] for tupe in pos_tagged_tokens})\n",
    "# dollar signs make everything go wrong\n",
    "#tags = [item.replace('$','x')for item in tags]\n",
    "badtags = ['#','$',',','-NONE-','.',':','TO','POS',\"''\"] # try to get NONE back if possible\n",
    "tags = [item for item in tags if item not in badtags]\n",
    "for tag in tags:\n",
    "#     if tag in ['#','$',',','-NONE-','.',':']:\n",
    "#         continue\n",
    "#     else:\n",
    "    allsyms = [('\\'' + tupe[0] + '\\'') for tupe in pos_tagged_tokens if tupe[1]== tag]\n",
    "    gr_rule = (tag + \" -> \")\n",
    "    gr_rule += ' | '.join(allsyms)\n",
    "    gr_rule += '\\n'\n",
    "    gr_rule = gr_rule.replace('PRP$','PRPx')\n",
    "    gr_rule = gr_rule.replace('WP$','WPx')\n",
    "    gr_rule = gr_rule.replace('-NONE-','xNONEx')\n",
    "    file.write(gr_rule)\n",
    "\n",
    "# specify these guys when creating grammar\n",
    "# still need to work with apostrophe, hypens ``\n",
    "file.write('''xperiod -> '.'\\n''')\n",
    "file.write('''xcomma -> ','\\n''')\n",
    "file.write('''xcolon -> ':'\\n''')\n",
    "file.write('''xsemicolon -> ';'\\n''')\n",
    "file.write('''openparen -> '('\\n''')\n",
    "file.write('''closeparen -> ')'\\n''')\n",
    "file.write('''xapostrophe -> \"\\'\"\\n''')\n",
    "file.write('''xquote -> \"''\"\\n''')\n",
    "\n",
    "# Save some compiling time\n",
    "file.write('''TO -> 'to'\\n''')\n",
    "\n",
    "# need to fix this\n",
    "#file.write('''POS -> \"\\'s\"\\n''')\n",
    "\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Pick an random sentence from corpus and print syntax tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We accordingly lay to,  hoping that some change would take place in the atmosphere and weather.\n",
      "SBARxS -> NP VP\n",
      "NP -> PRP\n",
      "PRP -> 'we'\n",
      "VP -> ADVP VB SxVP\n",
      "ADVP -> RB\n",
      "RB -> 'accordingly'\n",
      "VB -> 'lay'\n",
      "SxVP -> TO VP\n",
      "TO -> 'to'\n",
      "VP -> xcomma SxVP\n",
      "xcomma -> 'xcomma'\n",
      "SxVP -> VBG SBAR\n",
      "VBG -> 'hoping'\n",
      "SBAR -> IN S\n",
      "IN -> 'that'\n",
      "S -> NP VP xperiod\n",
      "NP -> DT NN\n",
      "DT -> 'some'\n",
      "NN -> 'change'\n",
      "VP -> MD VB NP\n",
      "MD -> 'would'\n",
      "VB -> 'take'\n",
      "NP -> NP PP\n",
      "NP -> NN\n",
      "NN -> 'place'\n",
      "PP -> IN NP\n",
      "IN -> 'in'\n",
      "NP -> NP CC NN\n",
      "NP -> DT NN\n",
      "DT -> 'the'\n",
      "NN -> 'atmosphere'\n",
      "CC -> 'and'\n",
      "NN -> 'weather'\n",
      "xperiod -> 'xperiod'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import CFG, ChartParser\n",
    "from random import choice\n",
    "\n",
    "from stat_parser import Parser\n",
    "parser = Parser()\n",
    "# pick a random sentence to parse\n",
    "# leave out the period at the end of the sentence\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "rsent = choice(tokenizer.tokenize(corpus))\n",
    "print (rsent)\n",
    "parsee=parser.parse(rsent)\n",
    "\n",
    "rules = \"\"\n",
    "to_replace = [',','.',':',';',\"''\",'(',')','$','+']\n",
    "replacements = ['xcomma','xperiod','xcolon','xsemicolon',\\\n",
    "                'xquote','openparen','closeparen','x','x']\n",
    " \n",
    "# possibly add: brackets, double quotes\n",
    "\n",
    "for production in parsee.productions():\n",
    "    rules += str(production) + '\\n'\n",
    "\n",
    "# now re-tag special characters\n",
    "swappairs = zip(to_replace, replacements)\n",
    "for member in swappairs:\n",
    "    rules = rules.replace(member[0],member[1])\n",
    "\n",
    "print (rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run grammar model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pick the nonterminal grammar rules that you want to use. Usually these need to be manually made by parsing single sentences and removing torublesome characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rules_file = 'corpora_and_rules/ALL_NT_RULES/homemade_NTrules.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk import CFG, ChartParser, Nonterminal\n",
    "from random import choice\n",
    "\n",
    "def is_terminal(item):\n",
    "    \"\"\"\n",
    "    Return true a symbol (str) is terminal\n",
    "    \"\"\"\n",
    "    return hasattr(item, '__hash__') and not isinstance(item, Nonterminal)\n",
    "\n",
    "def produce(grammar, symbol):\n",
    "    words = []\n",
    "    productions = grammar.productions(lhs = symbol)\n",
    "    production = choice(productions)\n",
    "    for sym in production.rhs():\n",
    "        if is_terminal(sym):\n",
    "            words.append(sym)\n",
    "        # recursion\n",
    "        else:\n",
    "            words.extend(produce(grammar, sym))\n",
    "    return words\n",
    "\n",
    "# read nonterminal rules from treebank file\n",
    "file = open(rules_file, 'r')\n",
    "cfg_str = file.read()\n",
    "file.close()\n",
    "\n",
    "# read leaves from corpus\n",
    "file = open(filename_root+\"_rules.txt\", 'r')\n",
    "cfg_str += '\\n' + file.read()\n",
    "file.close()\n",
    "\n",
    "grammar = CFG.fromstring(cfg_str)\n",
    "\n",
    "parser = ChartParser(grammar)\n",
    "\n",
    "gr = parser.grammar()\n",
    "\n",
    "# make an S symbol to start the fun\n",
    "tgr = ChartParser(CFG.fromstring('''S -> NP VP''')).grammar()\n",
    "tgr.start()\n",
    "# gr.max_len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate some random sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sailors rested that it that enemies of the imagination\n",
      "feelings often , deathbed although first fiend in but past of myself .\n",
      "which did few fate , forward to not was the same university , until this horror was the nature for me : the wretchedness the building with world along the moral proposition , the mountain of I against Beware each trade is creature of Hampden slaughter- . : your meal is content combat with her mother .\n",
      "which to taste I , again dated and pleasing of him on dearest with I : The letter as This miserable arm upon The reflection of its thousand and the death of the wood make called by the murderer : her fearful much opened nature , in the north religion and the progress , and to your pain , Nature In been into Below .\n",
      "Have is dictate as breathless\n",
      "next kindness her black , light of It I wished as our perish certainty .\n",
      "horrible wretch that memory , their lake I to me , which the glad unhappiness of the paroxysm of a several engagement of Clerval in her apartment and all mainland , and on a progress , me surprised so divine me .\n",
      "these conversation possessed to his time\n",
      "those sense all fulfillment , me of affairs a soul been of I .\n",
      "which my sun that this fear my loss the mate of and This rational rule useless , forever to bestowed not be return : your journey , horrors with their change the mind very was those blind corpse , which to heard that my soft actually considered teach . : expressive kind wretched of her tale .\n"
     ]
    }
   ],
   "source": [
    "#message = produce(gr, tgr.start())\n",
    "for ii in xrange(10):\n",
    "    print ' '.join(produce(gr, tgr.start())) \n",
    "\n",
    "\n",
    "# # tokenize by sentence and pick some random sentences to use?\n",
    "# # really need to get punctuation working\n",
    "# from stat_parser import Parser\n",
    "# parser = Parser()\n",
    "# kk=parser.parse(\"At the end of the nineteenth century the well-tested mechanical principles of Isaac Newton were the very heart of physical theory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a grammar file from Treebank"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# problem: searching for only identifiable tags is dropping clause symbols, etc. \n",
    "# But if a terminal symbo is missing, an IndexError will be raised\n",
    "# instead just make an lhs list and check for membership?\n",
    "\n",
    "rule_list = ''\n",
    "gram_leaves = [loc.rhs() for loc in list(tbank_productions)]\n",
    "gram_heads = [loc.lhs() for loc in list(tbank_productions)]\n",
    "gram_list= list(tbank_productions)\n",
    "for thing in gram_list:\n",
    "    thing_head = thing.lhs()\n",
    "    thing_leave = thing.rhs()\n",
    "    if str(thing_head) in tags:\n",
    "        #print str(thing_head)\n",
    "        continue\n",
    "    else:\n",
    "        message = str(thing_head) + \" -> \"\n",
    "        collection = [str(thing2) for thing2 in thing_leave]\n",
    "        for item in collection:\n",
    "            message += \" | \" + str(item)\n",
    "            rule_list += '\\n'+message\n",
    "\n",
    "    \n",
    "# # Get rid of dead heads\n",
    "# new_rule_list= ''\n",
    "# for line in rule_list.splitlines():\n",
    "#     if ' -> ' not in line[-5:]:\n",
    "#         new_rule_list +=line+'\\n'\n",
    "        \n",
    "rule_list=new_rule_list\n",
    "\n",
    "#get ride of first OR\n",
    "rule_list = rule_list.replace(' ->  |', ' ->  ')\n",
    "\n",
    "print rule_list\n",
    "# file = open('treebank_rules.txt', 'w')\n",
    "# file.write(rule_list)\n",
    "# file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parser = ChartParser(tbank_grammar)\n",
    "gr = parser.grammar()\n",
    "print ' '.join(produce(gr, gr.start()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
