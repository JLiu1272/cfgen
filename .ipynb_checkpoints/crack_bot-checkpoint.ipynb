{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-free grammar text generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk import data, CFG, ChartParser, Nonterminal\n",
    "from random import choice\n",
    "\n",
    "import string\n",
    "import glob\n",
    "import random\n",
    "# import cPickle as pickle\n",
    "# import pickle\n",
    "import os.path\n",
    "import warnings\n",
    "\n",
    "from stat_parser import Parser\n",
    "\n",
    "\n",
    "\n",
    "def clean_corpus(path):\n",
    "    '''\n",
    "    Clean up a corpus by removing characters and expressions that just aren't worth dealing with\n",
    "    \n",
    "    path : str\n",
    "        A string to a .txt file containing a corpus\n",
    "    \n",
    "    '''\n",
    "    filename_root = os.path.dirname(path)\n",
    "    corpus_members=glob.glob(path)\n",
    "    corpus = ''\n",
    "\n",
    "    # get rid of random line breaks and exclude troublesome expressions like quotes\n",
    "    for member in corpus_members:\n",
    "        with open (member, \"r\") as openfile:\n",
    "            data = openfile.read()\n",
    "            badchars = ['\\t','\\n','\\r','-\\n', '\\'','\\\"','`','|','--']\n",
    "#             badchars = ['\\t','\\n','\\r','-\\n', '|','--']\n",
    "            for badchar in badchars:\n",
    "                data = data.replace(badchar, ' ')\n",
    "            data = data.replace('.','.')\n",
    "            data = data.replace('`','\\'')\n",
    "            data = data.replace(',',',')\n",
    "            data = data.replace(';',';')\n",
    "            data = data.replace(':',':')\n",
    "        corpus = corpus + ' ' + data\n",
    "    \n",
    "    return corpus\n",
    "        \n",
    "def tag_corpus(corpus):\n",
    "    '''\n",
    "    Use NLTK to identify the linguisitic function of\n",
    "    the words in a corpus\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    corpus : str\n",
    "        A corpus that has been stripped of all troublesome\n",
    "        characters using the clean_corpus() function\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    pos_tagged_tokens : list of tuples\n",
    "        A list of tuples consisting of a word in position 1,\n",
    "        and its function within the sentence in position 2\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    tokens = nltk.word_tokenize(corpus)\n",
    "    pos_tagged_tokens = nltk.pos_tag(tokens)\n",
    "    \n",
    "    return pos_tagged_tokens\n",
    "\n",
    "def make_terminal_rules(pos_tagged_tokens):\n",
    "    '''\n",
    "    Search through a list of tagged words and obtain\n",
    "    all of the Terminal characters\n",
    "    \n",
    "    path : str\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    all_rules = ''\n",
    "    \n",
    "    tags = list({tupe[1] for tupe in pos_tagged_tokens})\n",
    "    badtags = ['#','$',',','-NONE-','.',':','TO','POS',\"''\",'(',')'] # bad terminal tags\n",
    "    tags = [item for item in tags if item not in badtags]\n",
    "    for tag in tags:\n",
    "        allsyms = [('\\'' + tupe[0] + '\\'') for tupe in pos_tagged_tokens if tupe[1]==tag]\n",
    "        gr_rule = (tag + \" -> \")\n",
    "        gr_rule += ' | '.join(allsyms)\n",
    "        gr_rule += '\\n'\n",
    "        gr_rule = gr_rule.replace('PRP$','PRPx')\n",
    "        gr_rule = gr_rule.replace('WP$','WPx')\n",
    "        gr_rule = gr_rule.replace('-NONE-','xNONEx')\n",
    "        all_rules += gr_rule\n",
    "\n",
    "    all_rules +=('''xperiod -> '.'\\n''')\n",
    "    all_rules +=('''xcomma -> ','\\n''')\n",
    "    all_rules +=('''xcolon -> ':'\\n''')\n",
    "    all_rules +=('''xsemicolon -> ';'\\n''')\n",
    "    all_rules +=('''openparen -> '('\\n''')\n",
    "    all_rules +=('''closeparen -> ')'\\n''')\n",
    "    all_rules +=('''xapostrophe -> \"\\'\"\\n''')\n",
    "    all_rules +=('''xquote -> \"''\"\\n''')\n",
    "    all_rules +=('''TO -> 'to'\\n''')\n",
    "    \n",
    "    return all_rules\n",
    "\n",
    "\n",
    "def parse_sentence(my_sentence):\n",
    "    '''\n",
    "    \n",
    "    my_sentence : str\n",
    "        A single sentence (str) \n",
    "    \n",
    "    '''\n",
    "       \n",
    "    parser = Parser()\n",
    "    parsee=parser.parse(my_sentence)\n",
    "\n",
    "    rules = \"\"\n",
    "    to_replace = [',','.',':',';',\"''\",'(',')','$','+']\n",
    "    replacements = ['xcomma','xperiod','xcolon','xsemicolon',\\\n",
    "                    'xquote','openparen','closeparen','xdollar','xplus']\n",
    "\n",
    "    # possibly add: brackets, double quotes\n",
    "\n",
    "    for production in parsee.productions():\n",
    "        rules += str(production) + '\\n'\n",
    "\n",
    "    # now re-tag special characters\n",
    "    swappairs = zip(to_replace, replacements)\n",
    "    for member in swappairs:\n",
    "        rules = rules.replace(member[0],member[1])\n",
    "\n",
    "    return rules\n",
    "\n",
    "\n",
    "def is_terminal(symb):\n",
    "    '''determine if a symbol is terminal\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    symb : str\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    out : bool\n",
    "        whether if symb is terminal\n",
    "\n",
    "    '''\n",
    "    out = hasattr(symb, '__hash__') and not isinstance(symb, Nonterminal)\n",
    "    return out\n",
    "\n",
    "\n",
    "def produce(grammar, symbol, depth=0, maxdepth=25):\n",
    "    '''\n",
    "    \n",
    "    grammar : nltk.grammar.CFG\n",
    "    \n",
    "    symbol : nltk.grammar.Nonterminal\n",
    "    \n",
    "    depth : int\n",
    "        The depth of the recursive tree search\n",
    "        \n",
    "    maxdepth : int\n",
    "        The maximum allowed recursion depth before throwing a\n",
    "        ValueError\n",
    "        \n",
    "    TODO: make a custom UserError type\n",
    "    \n",
    "    '''\n",
    "    if depth > maxdepth:\n",
    "        raise ValueError('Recursion went too deep, one of the example syntax sentences might be poorly formed or poorly parsed')\n",
    "    words = []\n",
    "    productions = grammar.productions(lhs = symbol)\n",
    "    production = choice(productions)\n",
    "    for sym in production.rhs():\n",
    "        if is_terminal(sym):\n",
    "            words.append(sym)\n",
    "            #print(depth)\n",
    "        else:\n",
    "            words.extend(produce(grammar, sym, depth=depth+1, maxdepth=maxdepth))\n",
    "    return words\n",
    "\n",
    "def make_sentence(cfg_str):\n",
    "    '''\n",
    "    cfg_str : str\n",
    "        a string containing a context free grammar\n",
    "        \n",
    "    '''\n",
    "\n",
    "\n",
    "    grammar = CFG.fromstring(cfg_str)\n",
    "    parser = ChartParser(grammar)\n",
    "    gr = parser.grammar()\n",
    "\n",
    "    startpt = cfg_str[:cfg_str.find(' ->')]\n",
    "    startpt = nltk.grammar.Nonterminal(startpt)\n",
    "    \n",
    "    out_txt = (' '.join(produce(gr, startpt)) )\n",
    "    \n",
    "\n",
    "    from_replace = [',','.',':',';',\"''\",'(',')','$','+']\n",
    "    replacements = ['xcomma','xperiod','xcolon','xsemicolon',\\\n",
    "                    'xquote','openparen','closeparen','xdollar','xplus']\n",
    "\n",
    "    # now re-tag special characters\n",
    "    swappairs = zip(replacements,from_replace)\n",
    "    for member in swappairs:\n",
    "        out_txt = out_txt.replace(member[0],member[1])\n",
    "    \n",
    "    \n",
    "    return out_txt\n",
    "\n",
    "\n",
    "def make_sentence2(corpus, term_rules, maxdepth=25):\n",
    "    '''\n",
    "    \n",
    "    Generate sentences with random structure and word choice\n",
    "    using a context-free grammar\n",
    "    \n",
    "    The start point is taken from the sentence itself.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    corpus : str\n",
    "        a string containing the full, cleaned corpus\n",
    "        \n",
    "    term_rules : str\n",
    "        a string containing all the terminal rules\n",
    "        \n",
    "    maxdepth : int\n",
    "        The maximum allowed recursion depth before throwing a\n",
    "        ValueError\n",
    "        \n",
    "    '''\n",
    "\n",
    "    #tgr = ChartParser(CFG.fromstring('''S -> NP VP''')).grammar()\n",
    "    #tgr.start()\n",
    "    \n",
    "    \n",
    "    flag = False\n",
    "    attempts = 0\n",
    "    while not flag and attempts < 30:\n",
    "        tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        rsent = choice(tokenizer.tokenize(corpus))\n",
    "        parsed_syntax = parse_sentence(rsent)\n",
    "        cfg_str = term_rules + parsed_syntax\n",
    "        try:  \n",
    "            startpt = parsed_syntax[:parsed_syntax.find(' ->')]\n",
    "            startpt = nltk.grammar.Nonterminal(startpt)\n",
    "            grammar = CFG.fromstring(cfg_str)\n",
    "            parser = ChartParser(grammar)\n",
    "            gr = parser.grammar()\n",
    "            out_txt = (' '.join(produce(gr, startpt,  maxdepth=maxdepth)) )\n",
    "            flag = True\n",
    "        except ValueError:\n",
    "            warnings.warn('Badly formed sentence encountered, resampling the corpus.')\n",
    "            attempts = attempts + 1\n",
    "\n",
    "    from_replace = [',','.',':',';',\"''\",'(',')','$','+']\n",
    "    replacements = ['xcomma','xperiod','xcolon','xsemicolon',\\\n",
    "                    'xquote','openparen','closeparen','xdollar','xplus']\n",
    "\n",
    "    # now re-tag special characters\n",
    "    swappairs = zip(replacements,from_replace)\n",
    "    for member in swappairs:\n",
    "        out_txt = out_txt.replace(member[0],member[1])\n",
    "    \n",
    "    \n",
    "    return out_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WHY DO SENTENCES SEEM TO STOP ABRUPTLY? LACK OF A PERIOD? CHECK THE STRUCTURE\n",
    "DEAL WITH EMPTY PRODUCTIONS IN GRAMMAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kk3 = ChartParser(CFG.fromstring('''S -> NP VP''')).grammar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.grammar.Nonterminal"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(kk3.start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.grammar.Nonterminal('S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'start'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-0f17222bee84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkk3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproductions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'start'"
     ]
    }
   ],
   "source": [
    "kk3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRPx\n",
      "the whole wretchedness became of The with the the and in shrill and lips as all impertinent account by the that dangerous but smiles of one cousin for that joy science of You human and friends But slaked which must desired by the with the the either arrived which can be replenished of the tried abhorrence . . .\n",
      "\n",
      "\n",
      "PRPx\n",
      "which magistrate ?\n",
      "\n",
      "\n",
      "PRPx\n",
      "When I bloomed , for all kindness although the boundless , of the love passed , of the aim of my father of I , If blood for the perceived The evil was hear to ten before us the man was dress to felt with I not right down kind and against I had despairing , that yourself drunk , but fallen now more innocent in the destruction probably yet forever sun and of him knew you , by the evening own , and passed I not more wide from I .\n",
      "\n",
      "\n",
      "PRPx\n",
      "monster was little from the steeples , the peaceful brightness were various of the cheerfulness\n",
      "\n",
      "\n",
      "PRPx\n",
      "you trembled hushed a effect for I She had young to was brought every triumph that promise\n",
      "\n",
      "\n",
      "PRPx\n",
      "little amusement .\n",
      "\n",
      "\n",
      "PRPx\n",
      "you destroyed so surprised , and loved me considered ready\n",
      "\n",
      "\n",
      "PRPx\n",
      "of my murderer There found my chain speedily For my joy which found There that were a .\n",
      "\n",
      "\n",
      "PRPx\n",
      "and who should tread the\n",
      "\n",
      "\n",
      "PRPx\n",
      "Elizabeth Britain .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ii in range(10):\n",
    "    print(make_sentence2(mycorp, termrules_mycorp))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When I recovered I found myself surrounded by the people of the inn; their countenances expressed a breathless terror, but the horror of others appeared only as a mockery, a shadow of the feelings that oppressed me.\n",
      "PRPx\n",
      ", But the time seen answer led that returns\n",
      "\n",
      "\n",
      "PRPx\n",
      "every a property for me was they\n",
      "\n",
      "\n",
      "PRPx\n",
      ", and I as I had vices\n",
      "\n",
      "\n",
      "PRPx\n",
      "I had the delight .\n",
      "\n",
      "\n",
      "PRPx\n",
      "which seemed looks : The happy appearance resolved I\n",
      "\n",
      "\n",
      "PRPx\n",
      "a the calmer in the misery stole we acquainted pursue softened off the poor s .\n",
      "\n",
      "\n",
      "PRPx\n",
      "the girl were the neighbourhood .\n",
      "\n",
      "\n",
      "PRPx\n",
      "She approved a snow went the business .\n",
      "\n",
      "\n",
      "PRPx\n",
      "him for These case which were anticipations\n",
      "\n",
      "\n",
      "PRPx\n",
      "how all family was longer of some dear sea . ; this horror sparkled he\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# random sentences with fixed grammar\n",
    "\n",
    "mycorp = clean_corpus('/Users/william/python_files/cfgen/full_books/frankenstein.txt')\n",
    "tagged_corpus = tag_corpus(mycorp)\n",
    "termrules_mycorp = make_terminal_rules(tagged_corpus)\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "rsent = choice(tokenizer.tokenize(mycorp))\n",
    "print (rsent)\n",
    "\n",
    "kk1 = parse_sentence(rsent)\n",
    "\n",
    "for ii in range(10):\n",
    "    print(make_sentence(termrules_mycorp+kk1))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "+ Remove capital letters on all words that start a sentence (identify with regular expression '\\. .'\n",
    "+ + Put them back into the final format using the same operation inverted\n",
    "+ + Need to identify words that are capital both when they start the sentence and whenever they appear (first names). Maybe make a list of all words that appear in both capital and non capital form?\n",
    "+ Fix the weird double period/comma problem, figure out where that's coming from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
