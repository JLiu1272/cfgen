{
 "metadata": {
  "name": "",
  "signature": "sha256:fb16ff0f169b9f421272205aa2ceadb96455289e249d01e6ae367603cd43d21d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Preamble"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "rcParams['font.family'] = 'DIN Pro'\n",
      "rcParams['font.weight'] = 'medium'\n",
      "# # These are the \"Tableau 20\" colors as RGB.  \n",
      "tableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),  \n",
      "             (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),  \n",
      "             (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),  \n",
      "             (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),  \n",
      "             (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]  \n",
      "tableau20 = [(0.59375, 0.257813, 0.886719),(0.800781, 0.0625, 0.460938), (0.278431, 0.788235, 0.478431), (0.917647, 0.682353, 0.105882), (0.372549, 0.596078, 1.), (0.8, 0.8, .8), (1.0, .3882, .2784)];\n",
      "for i in range(len(tableau20)):  \n",
      "    r, g, b = tableau20[i]  \n",
      "    tableau20[i] = (r, g, b)\n",
      "    # tableau20[i] = (r / 255., g / 255., b / 255.)  \n",
      "\n",
      "bcolor = 'Grey'\n",
      "\n",
      "rcParams['axes.color_cycle'] = tableau20\n",
      "rcParams['axes.labelcolor'] = bcolor\n",
      "rcParams['axes.edgecolor'] = bcolor\n",
      "rcParams['xtick.color'] = bcolor\n",
      "rcParams['ytick.color'] = bcolor\n",
      "\n",
      "rcParams['legend.frameon'] = False\n",
      "\n",
      "rcParams['axes.facecolor'] = 'none'\n",
      "\n",
      "rcParams['lines.linewidth'] = 2\n",
      "\n",
      "rcParams['figure.facecolor'] = 'none'\n",
      "rcParams['savefig.facecolor'] = 'none'\n",
      "\n",
      "#rcParams['figure.savefig.bbox']= 'none'\n",
      "\n",
      "plot([1, 3, 2, 3],[1, 2, 4 ,6], [5, 1 ,3, 5],[6, 2, 5, 1])\n",
      "savefig(\"out.pdf\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Markov comment bot"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import praw\n",
      "import string\n",
      "user_agent = (\"Data scraper 1.0 by /u/\")\n",
      "r = praw.Reddit(user_agent=user_agent)\n",
      "r.login('testbots','ertdfgCVB')\n",
      "\n",
      "# get a list of frequent subreddit users for the past six months\n",
      "srt_name = \"Physics\"\n",
      "srt = r.get_subreddit(srt_name)\n",
      "coms = srt.get_comments(limit=None)\n",
      "ct = 0;\n",
      "recent_names={};\n",
      "for comment in coms:\n",
      "    # don't get the deleted accounts since they're hard to work with\n",
      "    if comment.author is None:\n",
      "        continue\n",
      "    if comment.author.name in recent_names:\n",
      "        recent_names[comment.author.name] += 1\n",
      "    else:\n",
      "        recent_names[comment.author.name] = 1\n",
      "    ct += 1\n",
      "    if ct > 2000:\n",
      "        print \"stahp\"\n",
      "        break\n",
      "ords = sorted([(recent_names[x], x) for x in recent_names])\n",
      "all_top = ords[-25:]\n",
      "# A string list of the heaviest users\n",
      "top_users = [usr for (x, usr) in all_top]\n",
      "\n",
      "# fetch the top users in /r/physics\n",
      "topusrs = {}\n",
      "for usr in top_users:\n",
      "    topusrs[usr] = r.get_redditor(usr)\n",
      "\n",
      "# make an empty dict into which all their comments will be placed\n",
      "top_user_words = {}\n",
      "for usr in topusrs:\n",
      "    user = topusrs[usr]\n",
      "    ucom = user.get_submitted(limit=500)\n",
      "    comment_string = \"\"\n",
      "    for thing in ucom:\n",
      "        #flat_comments = praw.helpers.flatten_tree(thing.comments)\n",
      "        for comment in thing.comments:\n",
      "            comment_string = comment_string + \" \" + comment.body\n",
      "    # split string into words and assign\n",
      "    top_user_words[user.name] = comment_string.split()\n",
      "    break\n",
      "\n",
      "#\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# make an empty dict into which all the comments will be placed\n",
      "top_user_words = {}\n",
      "for usr in topusrs:\n",
      "    user = topusrs[usr]\n",
      "    ucom = user.get_submitted(limit=500)\n",
      "    comment_string = \"\"\n",
      "    for thing in ucom:\n",
      "        #flat_comments = praw.helpers.flatten_tree(thing.comments)\n",
      "        for comment in thing.comments:\n",
      "            comment_string = comment_string + \" \" + comment.body\n",
      "    # split string into words and assign\n",
      "    top_user_words[user.name] = comment_string.split()\n",
      "    break\n",
      "    \n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random\n",
      "\n",
      "#pick first from list\n",
      "kk = [topusrs[thing].name for thing in topusrs][0]\n",
      "\n",
      "bodytext = top_user_words[kk]\n",
      "seed_word = \"the\"\n",
      "message = seed_word\n",
      "while len(message) < 1000:\n",
      "    found_indices = [i for (i, x) in enumerate(bodytext) if x == seed_word]\n",
      "    next_words = [bodytext[index + 1] for index in found_indices]\n",
      "    next_word = random.choice(next_words)\n",
      "    message = message +' '+next_word\n",
      "    seed_word = next_word\n",
      "\n",
      "print message"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "un = \"wil3\"\n",
      "usr = r.get_redditor(un)\n",
      "thing_limit=None\n",
      "gen = usr.get_submitted(limit=thing_limit)\n",
      "karm = {}\n",
      "for thing in gen:\n",
      "    sr = thing.subreddit.display_name\n",
      "    karm[sr] = (karm.get(sr,0)+thing.score)\n",
      "#print dir(karm)\n",
      "for key in karm:\n",
      "    print key,\n",
      "    print karm[key]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# \n",
      "import praw\n",
      "\n",
      "# declare a helper function with optional args\n",
      "def bleep(*args):\n",
      "    print \"done\"\n",
      "\n",
      "# can sandbox bots using /r/test\n",
      "r = praw.Reddit('Comment scraper 1.0 by /u/testbots, account associated with /u/wil3')\n",
      "r.login('testbots','ertdfgCVB')\n",
      "submission = r.get_submission(submission_id='2a12v3')\n",
      "#comment_forest = submission.comments\n",
      "flat_comments = praw.helpers.flatten_tree(submission.comments)\n",
      "#print flat_comments\n",
      "# creates an empty set data structure\n",
      "already_done = set()\n",
      "bleep()\n",
      "for comment in flat_comments:\n",
      "    # avoid throwing an exception if the comment doesn't have the queried attribute\n",
      "    try:\n",
      "        if comment.body == \"Hello\" and comment.id not in already_done:\n",
      "            print \"here\"\n",
      "            #comment.reply(' world!')\n",
      "            print \"here\"\n",
      "            already_done.add(comment.id)\n",
      "    except AttributeError:\n",
      "        pass\n",
      "        \n",
      "        \n",
      "\n",
      "        #srt_name = \"Physics\"\n",
      "        #srt = r.get_subreddit(srt)\n",
      "        #srt_coms = srt.get_comments()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Import and clean a corpus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import praw\n",
      "# import string\n",
      "# import glob\n",
      "# import random\n",
      "  \n",
      "# path = 'victorian_corpus/*.txt'   \n",
      "# corpus_members=glob.glob(path)\n",
      "# corpus = ''\n",
      "# for member in corpus_members:\n",
      "#     with open (member, \"r\") as openfile:\n",
      "#         data = openfile.read()\n",
      "#         for badchar in ['\\t','\\n','- ','-\\n','(',')','[',']','\"','\\'','`','|']:\n",
      "#             data = data.replace(badchar, '')\n",
      "#         data.replace('.','. ')\n",
      "#     corpus = corpus + ' '+ data\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 107
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "One deep Markov Model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bodytext = corpus.split()\n",
      "seed_word = \"this\"\n",
      "message = seed_word # use word from thread found in corpus\n",
      "while len(message) < 1000:\n",
      "    found_indices = [i for (i, x) in enumerate(bodytext) if x == seed_word]\n",
      "    next_words = [bodytext[index + 1] for index in found_indices]\n",
      "    next_word = random.choice(next_words)\n",
      "    message = message +' '+next_word\n",
      "    seed_word = next_word\n",
      "\n",
      "print(message)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Clean and tag a corpus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "import string\n",
      "import glob\n",
      "import random\n",
      "import cPickle as pickle\n",
      "  \n",
      "path = 'frank/frank.txt'   \n",
      "corpus_members=glob.glob(path)\n",
      "corpus = ''\n",
      "# get rid of random line breaks and exclude troublesome expressions like quotes\n",
      "for member in corpus_members:\n",
      "    with open (member, \"r\") as openfile:\n",
      "        data = openfile.read()\n",
      "        for badchar in ['\\t','\\n','-\\n','\\'','\\\"','`','|','--']:\n",
      "            data = data.replace(badchar, ' ')\n",
      "        data = data.replace('.','. ')\n",
      "        data = data.replace(',',', ')\n",
      "        data = data.replace(';','; ')\n",
      "        data = data.replace(':',': ')\n",
      "    corpus = corpus + ' '+ data\n",
      "tokens = nltk.word_tokenize(corpus)\n",
      "\n",
      "# looks at each word in the context of sentence and tags it\n",
      "pos_tagged_tokens = nltk.pos_tag(tokens)\n",
      "pickle.dump( pos_tagged_tokens, open( \"frank/frank_tagged.pkl\", \"wb\" ) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 157
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "populate leaves of tree"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('frank/frank_tagged.pkl', 'rb') as handle:\n",
      "    pos_tagged_tokens = pickle.load(handle)\n",
      "\n",
      "# clear file\n",
      "open('frank/frank_rules.txt', 'w').close()\n",
      "\n",
      "file = open('frank/frank_rules.txt', 'a')\n",
      "\n",
      "# need to come up with some valid rules\n",
      "# tree = '''S -> NP VP\n",
      "# PP -> IN NP\n",
      "# NP -> DT NN | DT NN PP\n",
      "# VP -> VB NP | VP PP \\n'''\n",
      "# file.write(tree)\n",
      "\n",
      "tags = list({tupe[1] for tupe in pos_tagged_tokens})\n",
      "# dollar signs make everything go wrong\n",
      "#tags = [item.replace('$','x')for item in tags]\n",
      "badtags = ['#','$',',','-NONE-','.',':','TO','POS',\"''\"] # try to get NONE back if possible\n",
      "tags = [item for item in tags if item not in badtags]\n",
      "for tag in tags:\n",
      "#     if tag in ['#','$',',','-NONE-','.',':']:\n",
      "#         continue\n",
      "#     else:\n",
      "    allsyms = [('\\'' + tupe[0] + '\\'') for tupe in pos_tagged_tokens if tupe[1]== tag]\n",
      "    gr_rule = (tag + \" -> \")\n",
      "    gr_rule += ' | '.join(allsyms)\n",
      "    gr_rule += '\\n'\n",
      "    gr_rule = gr_rule.replace('PRP$','PRPx')\n",
      "    gr_rule = gr_rule.replace('WP$','WPx')\n",
      "    gr_rule = gr_rule.replace('-NONE-','xNONEx')\n",
      "    file.write(gr_rule)\n",
      "\n",
      "# specify these guys when creating grammar\n",
      "# still need to work with apostrophe, hypens ``\n",
      "file.write('''xperiod -> '.'\\n''')\n",
      "file.write('''xcomma -> ','\\n''')\n",
      "file.write('''xcolon -> ':'\\n''')\n",
      "file.write('''xsemicolon -> ';'\\n''')\n",
      "file.write('''openparen -> '('\\n''')\n",
      "file.write('''closeparen -> ')'\\n''')\n",
      "file.write('''xapostrophe -> \"\\'\"\\n''')\n",
      "file.write('''xquote -> \"''\"\\n''')\n",
      "\n",
      "# Save some compiling time\n",
      "file.write('''TO -> 'to'\\n''')\n",
      "\n",
      "# need to fix this\n",
      "#file.write('''POS -> \"\\'s\"\\n''')\n",
      "\n",
      "\n",
      "file.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 158
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Run grammar model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# gramr = nltk.parse_cfg(\"\"\"S -> NP VP\n",
      "# ... PP -> P NP\n",
      "# ... NP -> Det N | Det N PP | 'I'\n",
      "# ... VP -> V NP | VP PP\n",
      "# ... Det -> 'an' | 'my'\n",
      "# ... N -> 'elephant' | 'pajamas'\n",
      "# ... V -> 'shot'\n",
      "# ... P -> 'in'\n",
      "# ... \"\"\")\n",
      "\n",
      "import nltk\n",
      "\n",
      "from nltk import parse_cfg, ChartParser\n",
      "from random import choice\n",
      "\n",
      "def produce(grammar, symbol):\n",
      "    words = []\n",
      "    productions = grammar.productions(lhs = symbol)\n",
      "    production = choice(productions)\n",
      "    for sym in production.rhs():\n",
      "        if isinstance(sym, str):\n",
      "            words.append(sym)\n",
      "        # recursion\n",
      "        else:\n",
      "#             print sym\n",
      "            words.extend(produce(grammar, sym))\n",
      "    return words\n",
      "\n",
      "# read nonterminal rules from treebank file\n",
      "file = open('homemade_rules.txt', 'r')\n",
      "cfg_str = file.read()\n",
      "file.close()\n",
      "\n",
      "# read leaves from corpus\n",
      "file = open('frank/frank_rules.txt', 'r')\n",
      "cfg_str += '\\n' + file.read()\n",
      "file.close()\n",
      "\n",
      "grammar = parse_cfg('''\n",
      "S -> NP',' VP\n",
      "PP -> IN NP\n",
      "NP -> DT NN | DT NN PP | 'I'\n",
      "VP -> V NP | VP PP\n",
      "V -> 'shot' | 'killed' | 'wounded'\n",
      "DT -> 'an' | 'my'\n",
      "NN -> 'elephant'\n",
      "H -> 'unused'\n",
      "IN -> 'in' | 'outside'\n",
      "''')\n",
      "\n",
      "grammar = parse_cfg(cfg_str)\n",
      "\n",
      "parser = ChartParser(grammar)\n",
      "\n",
      "gr = parser.grammar()\n",
      "\n",
      "# make an S symbol to start the fun\n",
      "tgr = ChartParser(parse_cfg('''S -> NP VP''')).grammar()\n",
      "tgr.start()\n",
      "# gr.max_len()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 190,
       "text": [
        "S"
       ]
      }
     ],
     "prompt_number": 190
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#message = produce(gr, tgr.start())\n",
      "for ii in xrange(10):\n",
      "    print ' '.join(produce(gr, tgr.start())) \n",
      "\n",
      "\n",
      "# # tokenize by sentence and pick some random sentences to use?\n",
      "# # really need to get punctuation working\n",
      "# from stat_parser import Parser\n",
      "# parser = Parser()\n",
      "# kk=parser.parse(\"At the end of the nineteenth century the well-tested mechanical principles of Isaac Newton were the very heart of physical theory.\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "its history and first avenge , a half-finished misery scarcely dust the care .\n",
        "which to been instantly be me of I , scarcely appeared confess grief at the countenance as A immediate path : their hurricane only , employed for instant angel . : the countenance to mean loss than my distinct so occupied sea during I and the strength by all unwillingness untamed of busy power .\n",
        "me pronounced boat event that had visits have that drifting towards\n",
        "miserable fire picture the permit , that the patriot my first really transported mind in the such knowledge but the happiness by a frightful mark my insuperable easily accustomed evening precipitate produced of my benevolence\n",
        "The as these art that scenes described as a aspect been and combined\n",
        "all delight not , spoke for tears unknown and conceived .\n",
        "my beautiful not hired inquietude of immeasurable serenity these beautiful care , my variable indeed enslaved sorrow this fellow , that remained eyes with this willow endeavour in the courage of the truth before interested daylight for towards .\n",
        "every image but a friend , the happiness momentarily love his drenched much concealed anxiety on their old country .\n",
        "dreary mind had arrived been rather remonstrate benevolent justice if the especial return\n",
        "the fortitude often , certainty personal wreck .\n"
       ]
      }
     ],
     "prompt_number": 191
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "from nltk import parse_cfg, ChartParser\n",
      "from random import choice\n",
      "\n",
      "from stat_parser import Parser\n",
      "parser = Parser()\n",
      "# pick a random sentence to parse\n",
      "# leave out the period at the end of the sentence\n",
      "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "rsent = choice(tokenizer.tokenize(corpus))\n",
      "print rsent\n",
      "parsee=parser.parse(rsent)\n",
      "\n",
      "rules = \"\"\n",
      "to_replace = [',','.',':',';',\"''\",'(',')','$','+']\n",
      "replacements = ['xcomma','xperiod','xcolon','xsemicolon',\\\n",
      "                'xquote','openparen','closeparen','x','x']\n",
      " \n",
      "# possibly add: brackets, double quotes\n",
      "\n",
      "for production in parsee.productions():\n",
      "    rules += str(production) + '\\n'\n",
      "\n",
      "# now re-tag special characters\n",
      "swappairs = zip(to_replace, replacements)\n",
      "for member in swappairs:\n",
      "    rules = rules.replace(member[0],member[1])\n",
      "\n",
      "print rules\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "One secret which I alone possessed was the hope to which I had dedicated myself;  and the moon gazed on my midnight labours,  while,  with unrelaxed and breathless eagerness,  I pursued nature to her hiding-places.\n",
        "SQxFRAG -> ADJP xperiod\n",
        "ADJP -> ADJP SBAR\n",
        "ADJP -> CD NN\n",
        "CD -> 'one'\n",
        "NN -> 'secret'\n",
        "SBAR -> WHNP S\n",
        "WHNP -> WDT\n",
        "WDT -> 'which'\n",
        "S -> NP VP\n",
        "NP -> PRP\n",
        "PRP -> 'I'\n",
        "VP -> ADVP VBN VBD SBARxS\n",
        "ADVP -> RB\n",
        "RB -> 'alone'\n",
        "VBN -> 'possessed'\n",
        "VBD -> 'was'\n",
        "SBARxS -> NP VP\n",
        "NP -> DT NN\n",
        "DT -> 'the'\n",
        "NN -> 'hope'\n",
        "VP -> PP SBAR\n",
        "PP -> TO NP\n",
        "TO -> 'to'\n",
        "NP -> SBAR xcomma\n",
        "SBAR -> WHNP S\n",
        "WHNP -> WDT\n",
        "WDT -> 'which'\n",
        "S -> NP VP\n",
        "NP -> PRP\n",
        "PRP -> 'I'\n",
        "VP -> VBD VBD SBARxS\n",
        "VBD -> 'had'\n",
        "VBD -> 'dedicated'\n",
        "SBARxS -> NP VP\n",
        "NP -> JJ xcolon CC DT NN\n",
        "JJ -> 'myself'\n",
        "xcolon -> 'xsemicolon'\n",
        "CC -> 'and'\n",
        "DT -> 'the'\n",
        "NN -> 'moon'\n",
        "VP -> VBN PP\n",
        "VBN -> 'gazed'\n",
        "PP -> IN NP\n",
        "IN -> 'on'\n",
        "NP -> PRPx JJ NN\n",
        "PRPx -> 'my'\n",
        "JJ -> 'midnight'\n",
        "NN -> 'labours'\n",
        "xcomma -> 'xcomma'\n",
        "SBAR -> IN S\n",
        "IN -> 'while'\n",
        "S -> xcomma PP xcomma NP VP\n",
        "xcomma -> 'xcomma'\n",
        "PP -> IN NP\n",
        "IN -> 'with'\n",
        "NP -> JJ CC JJ NN\n",
        "JJ -> 'unrelaxed'\n",
        "CC -> 'and'\n",
        "JJ -> 'breathless'\n",
        "NN -> 'eagerness'\n",
        "xcomma -> 'xcomma'\n",
        "NP -> PRP\n",
        "PRP -> 'I'\n",
        "VP -> NP PP\n",
        "NP -> JJ NN\n",
        "JJ -> 'pursued'\n",
        "NN -> 'nature'\n",
        "PP -> TO NP\n",
        "TO -> 'to'\n",
        "NP -> PRPx NN\n",
        "PRPx -> 'her'\n",
        "NN -> 'hiding-places'\n",
        "xperiod -> 'xperiod'\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 183
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "create a grammar file from Treebank"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "from nltk.corpus import treebank\n",
      "from nltk.grammar import ContextFreeGrammar, Nonterminal\n",
      "\n",
      "tbank_productions = set(production for sent in treebank.parsed_sents()\n",
      "                        for production in sent.productions())\n",
      "tbank_grammar = ContextFreeGrammar(Nonterminal('S'), list(tbank_productions))\n",
      "\n",
      "allsyms = {thing.lhs() for thing in tbank_grammar.productions()}\n",
      "rule_list = ''\n",
      "for sym in allsyms:\n",
      "    if str(sym) not in tags:\n",
      "        if str(sym) not in badtags:\n",
      "            rules_sym = tbank_grammar.productions(lhs = sym)\n",
      "            for rule in rules_sym:\n",
      "                strule = str(rule)\n",
      "                strule = strule.replace('PRP$','PRPx')\n",
      "                strule = strule.replace('WP$','WPx')\n",
      "                strule = strule.replace('ADVP|PRT','ADVPxPRT')\n",
      "                strule = strule.replace('ADVP-PRD-LOC=3','ADVP-PRD-LOCx3')\n",
      "                strule = strule.replace('-LRB-','xLRBx')\n",
      "                strule = strule.replace('-RRB-','xRRBx')\n",
      "                strule = strule.replace('-NONE-','xNONEx') # need to update leaves as well\n",
      "                rule_list += strule + '\\n'\n",
      "    else:   \n",
      "        continue\n",
      "print rule_list\n",
      "file = open('treebank_rules.txt', 'w')\n",
      "file.write(rule_list)\n",
      "file.close()\n",
      "\n",
      "\n",
      "# need to deal with these special characters:\n",
      "#  -NONE- ``  .    -LRB- with LRB -RRB- '' :  \n",
      "# ADVP-PRD-LOC=3 ADJP-PRD1 $ #\n",
      "#  -> \n",
      "# -> '`'\n",
      "# -> \"\"\n",
      "# -> \"'\"\n",
      "# maybe try 'xCOMMAx'. Issue is that the symbol , has be be nonterminal (ie, it has to point to a terminal)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "file = open('stringtest.txt', 'r')\n",
      "print file.read()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ",\n",
        "``\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The uniform from the energy represent the appearance because the whole\n",
      "\n",
      "the due condition source die as own orbit Parts and arises of the aether is the destiny. He system time a following arc now the a\u00bb At heart part more though a varying orbital page conclusion referred the opposite problem believes was more causally"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "# problem: searching for only identifiable tags is dropping clause symbols, etc. \n",
      "# But if a terminal symbo is missing, an IndexError will be raised\n",
      "# instead just make an lhs list and check for membership?\n",
      "\n",
      "rule_list = ''\n",
      "gram_leaves = [loc.rhs() for loc in list(tbank_productions)]\n",
      "gram_heads = [loc.lhs() for loc in list(tbank_productions)]\n",
      "gram_list= list(tbank_productions)\n",
      "for thing in gram_list:\n",
      "    thing_head = thing.lhs()\n",
      "    thing_leave = thing.rhs()\n",
      "    if str(thing_head) in tags:\n",
      "        #print str(thing_head)\n",
      "        continue\n",
      "    else:\n",
      "        message = str(thing_head) + \" -> \"\n",
      "        collection = [str(thing2) for thing2 in thing_leave]\n",
      "        for item in collection:\n",
      "            message += \" | \" + str(item)\n",
      "            rule_list += '\\n'+message\n",
      "\n",
      "    \n",
      "# # Get rid of dead heads\n",
      "# new_rule_list= ''\n",
      "# for line in rule_list.splitlines():\n",
      "#     if ' -> ' not in line[-5:]:\n",
      "#         new_rule_list +=line+'\\n'\n",
      "        \n",
      "rule_list=new_rule_list\n",
      "\n",
      "#get ride of first OR\n",
      "rule_list = rule_list.replace(' ->  |', ' ->  ')\n",
      "\n",
      "print rule_list\n",
      "# file = open('treebank_rules.txt', 'w')\n",
      "# file.write(rule_list)\n",
      "# file.close()\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "parser = ChartParser(tbank_grammar)\n",
      "gr = parser.grammar()\n",
      "print ' '.join(produce(gr, gr.start()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "NGram 3 markov"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "tokens = nltk.word_tokenize(corpus)\n",
      "# tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+|[^\\w\\s]+')\n",
      "# content_text = ' '.join(article.content for article in articles)\n",
      "# tokenized_content = tokenizer.tokenize(content_text)\n",
      "content_model = nltk.NgramModel(2, tokens)\n",
      "\n",
      "#starting_words = content_model.generate(100)[-2:]\n",
      "#content = content_model.generate(words_to_generate, starting_words)\n",
      "#print ' '.join(content)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 199
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# will throw an error for small corpus\n",
      "robotext = content_model.generate(100,'This')\n",
      "print ' '.join(robotext)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 86 samples> sums to 0.5179293212281115; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 1 samples> sums to 0.3582920585555367; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 12 samples> sums to 0.3010968340140925; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 186 samples> sums to 0.9390083250809893; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 236 samples> sums to 0.7362707529206318; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 148 samples> sums to 0.619494780052439; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 3 samples> sums to 0.695272637323873; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 302 samples> sums to 0.6680326406726631; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 236 samples> sums to 0.7861922674700187; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 11 samples> sums to 0.2836050264010106; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 1635 samples> sums to 0.8187765735998777; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 3908 samples> sums to 0.8755396659910119; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 2 samples> sums to 0.6897185014632843; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 1345 samples> sums to 0.9468891488797238; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 5 samples> sums to 0.39148998461477236; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 1 samples> sums to 0.3272033699870952; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 20 samples> sums to 0.6387292965574733; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 11 samples> sums to 0.6250427862031722; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 546 samples> sums to 0.9390917095506107; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 4 samples> sums to 0.846753884807785; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 2 samples> sums to 0.44289185405690257; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 751 samples> sums to 0.902137284237809; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 17 samples> sums to 0.4981912022997689; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 30 samples> sums to 0.3329690568308621; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 970 samples> sums to 0.9859847773245114; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 30 samples> sums to 0.6587215162030653; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 14 samples> sums to 0.8799099873029482; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 22 samples> sums to 0.6974415833323815; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 2637 samples> sums to 0.968223958757137; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 14 samples> sums to 0.6891132524585208; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 1079 samples> sums to 0.9977962542610109; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 57 samples> sums to 0.6687850903227524; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 2637 samples> sums to 0.8163330636287127; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 173 samples> sums to 0.7971880767304437; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 20 samples> sums to 0.6553918642529108; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 176 samples> sums to 0.41453570050350164; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 3 samples> sums to 0.605219877384972; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 6 samples> sums to 0.8289757430184085; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 18 samples> sums to 0.8253977048198456; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "T h i s apprehensions as he had hitherto been present to your health rendered , no disaster is murdered , on your duty towards the most beautiful that you could I opened my , I , the reflections determined thenceforth to a direction , and my appetite. One by Elizabeth. She was not unfolded to recollect what has been adduced against me was free last night ; such a fiend can not describe their native town of great crime , in death shall be the beginning of food or take their inquiries clear conception of the dashing waves continually renewed violence except at\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 6 samples> sums to 0.1866970521909247; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n",
        "/Library/Python/2.7/site-packages/nltk/probability.py:598: UserWarning: Probability distribution <SimpleGoodTuringProbDist based on 11 samples> sums to 0.5961183808511158; generate() is returning an arbitrary sample.\n",
        "  \" is returning an arbitrary sample.\" % (self, 1-p))\n"
       ]
      }
     ],
     "prompt_number": 201
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "CFG from elsewhere: http://www.decontextualize.com/teaching/rwet/recursion-and-context-free-grammars/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class ContextFree(object):\n",
      "\n",
      "  def __init__(self):\n",
      "    self.rules = dict()\n",
      "    self.expansion = list()\n",
      "\n",
      "  # rules are stored in self.rules, a dictionary; the rules themselves are\n",
      "  # lists of expansions (which themselves are lists)\n",
      "  def add_rule(self, rule, expansion): \n",
      "    if rule in self.rules:\n",
      "      self.rules[rule].append(expansion)\n",
      "    else:\n",
      "      self.rules[rule] = [expansion]\n",
      "\n",
      "  def expand(self, start):\n",
      "\n",
      "    from random import choice\n",
      "\n",
      "    # if the starting rule was in our set of rules, then we can expand it \n",
      "    if start in self.rules:\n",
      "      possible_expansions = self.rules[start]\n",
      "      # grab one possible expansion\n",
      "      random_expansion = choice(possible_expansions)\n",
      "      # call this method again with the current element of the expansion\n",
      "      for elem in random_expansion:\n",
      "        self.expand(elem)\n",
      "    else:\n",
      "      # if the rule wasn't found, then it's a terminal: simply append the\n",
      "      # string to the expansion\n",
      "      self.expansion.append(start)\n",
      "\n",
      "  # utility method to run the expand method and return the results\n",
      "  def get_expansion(self, axiom):\n",
      "    self.expand(axiom)\n",
      "    return self.expansion\n",
      "\n",
      "if __name__ == '__main__':\n",
      "\n",
      "  cfree = ContextFree()\n",
      "  cfree.add_rule('S', ['NP', 'VP'])\n",
      "  cfree.add_rule('NP', ['the', 'N'])\n",
      "  cfree.add_rule('N', ['cat'])\n",
      "  cfree.add_rule('N', ['dog'])\n",
      "  cfree.add_rule('N', ['weinermobile'])\n",
      "  cfree.add_rule('N', ['duchess'])\n",
      "  cfree.add_rule('VP', ['V', 'the', 'N'])\n",
      "  cfree.add_rule('V', ['sees'])\n",
      "  cfree.add_rule('V', ['chases'])\n",
      "  cfree.add_rule('V', ['lusts after'])\n",
      "  cfree.add_rule('V', ['blames'])\n",
      "\n",
      "  expansion = cfree.get_expansion('S')\n",
      "  print ' '.join(expansion)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import context_free\n",
      "import re\n",
      "\n",
      "def add_rules_from_file(cfree, file_obj):\n",
      "  # rules are stored in the given file in the following format:\n",
      "  # Rule -> a | a b c | b c d\n",
      "  # ... which will be translated to:\n",
      "  # self.add_rule('Rule', ['a'])\n",
      "  # self.add_rule('Rule', ['a', 'b', 'c'])\n",
      "  # self.add_rule('Rule', ['b', 'c', 'd'])\n",
      "  for line in file_obj:\n",
      "    line = re.sub(r\"#.*$\", \"\", line) # get rid of comments\n",
      "    line = line.strip() # strip any remaining white space\n",
      "    match_obj = re.search(r\"(\\w+) *-> *(.*)\", line)\n",
      "    if match_obj:\n",
      "      rule = match_obj.group(1)\n",
      "      expansions = re.split(r\"\\s*\\|\\s*\", match_obj.group(2))\n",
      "      for expansion in expansions:\n",
      "        expansion_list = expansion.split(\" \")\n",
      "        cfree.add_rule(rule, expansion_list)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "\n",
      "  cfree = context_free.ContextFree()\n",
      "  add_rules_from_file(cfree, open(\"test.grammar\"))\n",
      "  expansion = cfree.get_expansion('S')\n",
      "  print ' '.join(expansion)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "' '.join(cfree.get_expansion('S'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}